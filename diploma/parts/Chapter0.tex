\chapter{Introduction}
\label{sec:Chapter0} \index{Chapter0}

Write Introduction. The expected size is one page. The expected plan is:
\begin{itemize}
    \item the research goal (and its motivations),
    \item the object of research (introduce main termini),
    \item the problem (what is the challenge),
    \item methodology: literature review and state-of-the-art,
    \item the project tasks,
    \item the proposed solution, its novelty, and advantages,
    \item the pros and cons of recent works,
    \item goal of the experiment, set up, data sets, workflow.
\end{itemize}

Three questions to answer:
\begin{itemize}
    \item What is the nearest alternative result?
    \item What is the advantage?
    \item What are the distinguished characteristics?
\end{itemize}

It follows the formula:
\emph{The paper proposed a method (for) X, providing Y, and distinguished by Z.}

In recent years, memory-augmented transformers (Memformers) have garnered significant attention for their ability to implement complex optimization algorithms. However, despite their potential, the question remains: can Memformers effectively learn and implement more advanced optimization methods, such as Linear First-Order Methods (LFOMs)? This work aims to explore the algorithmic capabilities of Memformers and their ability to learn and generalize complex optimization algorithms.

\section{Literature Review}
Research on transformers is rapidly evolving, and it is impossible to cover the entire breadth of related literature. However, we briefly summarize the most relevant topics below.

\textbf{In-context Learning.} The ability of transformer models to perform in-context learning (ICL) has been widely studied since its introduction \cite{NEURIPS2020_1457c0d6}. Subsequent works have explored how these models adapt to new tasks without requiring parameter updates \cite{xie2022explanationincontextlearningimplicit, vonoswald2024uncoveringmesaoptimizationalgorithmstransformers, hahn2023theoryemergentincontextlearning, liu2021makesgoodincontextexamples, lu2022fantasticallyorderedpromptsthem, NEURIPS2022_9d560961, wu2023selfadaptiveincontextlearninginformation}. This foundational research has paved the way for exploring how transformers can implement specific algorithms, such as gradient-based methods.

\textbf{Gradient-based Methods in Transformers.} \cite{garg2022can} analyze learning gradient descent in transformers, particularly in the context of ICL for linear functions. Empirical studies \cite{garg2022can, akyurek2022learning, pmlr-v202-von-oswald23a} have demonstrated that transformers can learn gradient descent after training on random linear regression tasks. Extending these results, \cite{pmlr-v202-von-oswald23a, ahn2023transformerslearnimplementpreconditioned} show that transformers can implement preconditioned gradient descent to solve linear regression problems presented in input prompts. Notably, these works, as well as ours, utilize linear transformers, as discussed in \cite{schlag2021lineartransformerssecretlyfast, pmlr-v202-von-oswald23a, ahn2023linear}.

\textbf{Higher-order Optimization Methods in Transformers.} Transformers have also demonstrated the ability to learn higher-order optimization methods, such as Newton's method, extending their capabilities beyond first-order methods \cite{fu2023transformerslearnachievesecondorder, giannou2024transformersemulateincontextnewtons, vladymyrov2024lineartransformersversatileincontext}.

\textbf{Memory-augmented Transformers (Memformers).} Memformers were introduced by \cite{wu2022memformermemoryaugmentedtransformersequence, Xu_Liang_Cheng_Wei_Chen_Zhang_2021, dutta2024memoryaugmentedtransformersimplementlinear}. These models retain intermediate attention values across layers using memory registers, enabling the exploration of more complex computations and optimization methods. While significant progress has been made in understanding how transformers can learn gradient descent, their potential for exploring more advanced LFOMs remains largely unexplored. Our work addresses this gap, demonstrating how Memformers can effectively implement a wide range of advanced first-order and quasi-second-order optimization methods, including conjugate gradient and momentum methods, thereby pushing the boundaries of transformer-based architectures.
