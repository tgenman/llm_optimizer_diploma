{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TIiF8VkplNsr"
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# This file contains the following:\n",
    "# 1. Linear Transformer Model\n",
    "# 2. Function for clipping gradient\n",
    "# 3. Function for generating random data\n",
    "###########################################\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Definition of a single linear attention unit for linear-regression data\n",
    "# P is the value matrix\n",
    "# Q is the product of key,query matrices\n",
    "# the dimensions of the input are\n",
    "# B: batch-size of prompts\n",
    "# N: context length (excluding query)\n",
    "# d: covariate dimension\n",
    "# P,Q are d x d matrices\n",
    "# Z is a B x (N+1) + (d+1) matrix\n",
    "# Output is also B x (N+1) + (d+1)\n",
    "\n",
    "# For linear attention, activation = None\n",
    "# For standard attention, activation(x) = torch.nn.functional.softmax(x, dim = 2)\n",
    "# For ReLU attention, activation(x) = torch.nn.relu(x)\n",
    "def attention(P,Q,Z, activation = None):\n",
    "    B= Z.shape[0]\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    P_full =  torch.cat([P,torch.zeros(1,d).to(device)],dim=0)\n",
    "    P_full =  torch.cat([P_full,torch.zeros(d+1,1).to(device)],dim=1)\n",
    "    P_full[d,d] = 1\n",
    "    Q_full = torch.cat([Q, torch.zeros(1,d).to(device)],dim=0)\n",
    "    Q_full = torch.cat([Q_full, torch.zeros(d+1,1).to(device)],dim=1)\n",
    "    A = torch.eye(N+1).to(device)\n",
    "    A[N,N] = 0\n",
    "    Attn = torch.einsum('BNi, ij, BMj -> BNM', (Z,Q_full,Z))\n",
    "    if activation is not None:\n",
    "        Attn = activation(Attn)\n",
    "    key = torch.einsum('ij, BNj -> BNi', (P_full,Z))\n",
    "    Output = torch.einsum('BNM,ML, BLi -> BNi', (Attn,A,key))\n",
    "    return Output /N\n",
    "\n",
    "\n",
    "# The Linear Transformer module\n",
    "# n_layer denotes the number of layers\n",
    "# n_head denotes the number of heads. In most of our experiments, n_head = 1\n",
    "# d denotes the dimension of covariates\n",
    "# var denotes the variance of initialization. It needs to be sufficiently small, but exact value is not important\n",
    "# allparam: contains all the parameters, has dimension n_layer x n_head x 2 x d x d\n",
    "# For example\n",
    "# - P matrix at layer i, head j is allparam[i,j,0,:,:]\n",
    "# - Q matrix at layer i, head j is allparam[i,j,1,:,:]\n",
    "# If run_mode = 0 then training. If run_mode = 1 then in-context learning.\n",
    "class Transformer_F(nn.Module):\n",
    "    def __init__(self, n_layer, n_head, d, var, run_mode):\n",
    "        super(Transformer_F, self).__init__()\n",
    "        self.register_parameter('allparam', torch.nn.Parameter(torch.zeros(n_layer, n_head, 2, d, d)))\n",
    "        # Initialize gamma with requires_grad=False initially\n",
    "        self.gamma = nn.Parameter(torch.zeros(n_layer), requires_grad=False)\n",
    "        self.alpha = nn.Parameter(torch.ones(n_layer), requires_grad=False)\n",
    "        with torch.no_grad():\n",
    "            self.allparam.normal_(0,var)\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.run_mode = run_mode\n",
    "\n",
    "    def forward(self, Z):\n",
    "        R = torch.zeros_like(Z)\n",
    "        # Toggle gamma training based on run_mode\n",
    "        self.gamma.requires_grad = (self.run_mode == 1)\n",
    "        # Extract B, N and d values\n",
    "        B, N, d = Z.shape[0], Z.shape[1]-1, Z.shape[2]-1\n",
    "\n",
    "        # Enforce the sparsity condition before processing\n",
    "        self.zero_p()\n",
    "        # Enforce the scalar multiple of identity condition on Q\n",
    "        self.identity_q()\n",
    "\n",
    "        for i in range(self.n_layer):\n",
    "            Zi = Z\n",
    "            attention_sum = torch.zeros_like(Z)\n",
    "            # the forwarad map of each layer is given by F(Z) = Z + attention(Z)\n",
    "            for j in range(self.n_head):\n",
    "                Pij = self.allparam[i,j,0,:,:]\n",
    "                Qij = self.allparam[i,j,1,:,:]\n",
    "                attention_sum = attention_sum + attention(Pij,Qij,Zi)\n",
    "\n",
    "            if self.run_mode == 0:\n",
    "                R = attention_sum\n",
    "            else:\n",
    "                R = R * self.gamma[i] + attention_sum\n",
    "            Z = Zi + R * self.alpha[i]\n",
    "        return Z\n",
    "\n",
    "    #enforces top-left-dxd-block sparsity on p\n",
    "    def zero_p(self):\n",
    "        for i in range(self.n_layer):\n",
    "            for j in range(self.n_head):\n",
    "                with torch.no_grad():\n",
    "                    self.allparam[i,j,0,:,:].zero_()\n",
    "\n",
    "    def identity_q(self):\n",
    "        with torch.no_grad():  # Disable gradient tracking\n",
    "            for i in range(self.n_layer):\n",
    "                for j in range(self.n_head):\n",
    "                    Qij = self.allparam[i,j,1,:,:]\n",
    "\n",
    "                    # Zero out all off-diagonal elements\n",
    "                    off_diag_mask = ~torch.eye(Qij.size(-1), dtype=torch.bool, device=Qij.device)\n",
    "                    Qij.masked_fill_(off_diag_mask, 0)\n",
    "\n",
    "                    # Set diagonal elements to the average of the original diagonal elements\n",
    "                    diag_elements = Qij.diag()\n",
    "                    diag_mean = diag_elements.mean()\n",
    "                    Qij.fill_diagonal_(diag_mean)\n",
    "\n",
    "                    # Safely copy the modified tensor back to allparam\n",
    "                    self.allparam[i,j,1,:,:].copy_(Qij)\n",
    "\n",
    "# evaluate the loss of model, given data (Z,y)\n",
    "def in_context_loss(model, Z, y):\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    output = model(Z)\n",
    "    diff = output[:,N,d]+y\n",
    "    loss = ((diff)**2).mean()\n",
    "    return loss\n",
    "\n",
    "# generate random data for linear regression\n",
    "# mode: distribution of samples to generate. Currently supports 'normal', 'gamma', 'sphere'\n",
    "# N: number of context examples\n",
    "# d: dimension of covariates\n",
    "# For gamma distribution:\n",
    "# - shape_k: shape parameter of gamma distribution (unused otherwise)\n",
    "# - scale parameter: hard coded so that when shape_k = 5/2 and d=5, the generated data is standard normal\n",
    "def generate_data(mode='normal',N=20,d=1,B=1000,shape_k=0.1, U=None, D=None):\n",
    "    W= torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
    "    X_test = torch.FloatTensor(B,1,d).normal_(0, 1).to(device)\n",
    "\n",
    "    if U is not None:\n",
    "        U = U.to(device)\n",
    "        D = D.to(device)\n",
    "        W= torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "        W = torch.mm(W,torch.inverse(D))\n",
    "        W = torch.mm(W,U.t())\n",
    "\n",
    "    if mode =='sphere':\n",
    "        X.div_(X.norm(p=2,dim=2)[:,:,None])\n",
    "        X_test.div_(X_test.norm(p=2,dim=2)[:,:,None])\n",
    "    elif mode == 'gamma':\n",
    "        # random gamma scaling for X\n",
    "        gamma_scales = np.random.gamma(shape=shape_k, scale=(10/shape_k)**(0.5), size=[B,N])\n",
    "        gamma_scales = torch.Tensor(gamma_scales).to(device)\n",
    "        gamma_scales = gamma_scales.sqrt()\n",
    "        # random gamma scaling for X_test\n",
    "        gamma_test_scales = np.random.gamma(shape=shape_k, scale=(10/shape_k)**(0.5), size=[B,1])\n",
    "        gamma_test_scales = torch.Tensor(gamma_test_scales).to(device)\n",
    "        gamma_test_scales = gamma_test_scales.sqrt()\n",
    "        # normalize to unit norm\n",
    "        X.div_(X.norm(p=2,dim=2)[:,:,None])\n",
    "        X_test.div_(X_test.norm(p=2,dim=2)[:,:,None])\n",
    "        # scale by gamma\n",
    "        X.mul_(gamma_scales[:,:,None])\n",
    "        X_test.mul_(gamma_test_scales[:,:,None])\n",
    "    elif mode =='normal':\n",
    "        assert True\n",
    "    elif mode == 'relu':\n",
    "        return generate_data_relu(N=N, d=d, B=B, hidden_dim=d)\n",
    "    elif mode == 'mlp':\n",
    "        generate_data_mlp(N=N, d=d, B=B, hidden_dim=d)\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    if U is not None:\n",
    "        X = torch.einsum('ij, jk, BNk -> BNi', (U,D,X))\n",
    "        X_test = torch.einsum('ij, jk, BNk -> BNi', (U,D,X_test))\n",
    "\n",
    "    y = torch.einsum('bi,bni->bn', (W, X)).unsqueeze(2)\n",
    "    y_zero = torch.zeros(B,1,1).to(device)\n",
    "    y_test = torch.einsum('bi,bni->bn', (W, X_test)).squeeze(1)\n",
    "    X_comb= torch.cat([X,X_test],dim=1)\n",
    "    y_comb= torch.cat([y,y_zero],dim=1)\n",
    "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
    "    return Z.to(device),y_test.to(device)\n",
    "\n",
    "def generate_data_inplace(Z, U=None, D=None):\n",
    "\n",
    "\n",
    "    B = Z.shape[0]\n",
    "    N = Z.shape[1]-1\n",
    "    d = Z.shape[2]-1\n",
    "    X = Z[:,:,0:-1]\n",
    "    X.normal_(0, 1).to(device)\n",
    "    W= torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "    if U is not None:\n",
    "        U = U.to(device)\n",
    "        D = D.to(device)\n",
    "        W = torch.mm(W,torch.inverse(D))\n",
    "        W = torch.mm(W,U.t())\n",
    "        Z[:,:,0:-1] = torch.einsum('ij, jk, BNk -> BNi', (U,D,X))\n",
    "\n",
    "    Z[:,:,-1] = torch.einsum('bi,bni->bn', (W, Z[:,:,0:-1])) #y update\n",
    "    y_test = Z[:,-1,-1].detach().clone()\n",
    "    Z[:,-1,-1].zero_()\n",
    "    return Z.to(device),y_test.to(device)\n",
    "\n",
    "def generate_data_sine(N=10, B=1000):\n",
    "    # Sample amplitude a and phase p for each task\n",
    "    a = torch.FloatTensor(B).uniform_(0.1, 5).to(device)\n",
    "    p = torch.FloatTensor(B).uniform_(0, math.pi).to(device)\n",
    "\n",
    "    X = torch.FloatTensor(B, N).uniform_(-5, 5).to(device)\n",
    "\n",
    "    Y = a.unsqueeze(1) * torch.sin(p.unsqueeze(1) + X)\n",
    "\n",
    "    X = X.unsqueeze(-1)\n",
    "    Y = Y.unsqueeze(-1)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def generate_data_relu(mode='normal', N=20, d=1, B=1000, shape_k=0.1, U=None, D=None, hidden_dim=100):\n",
    "    # Generate random input data\n",
    "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
    "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
    "\n",
    "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
    "\n",
    "    # Define a 1-hidden layer ReLU network\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(d, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, 1)\n",
    "    ).to(device)\n",
    "    model[0].weight.data.normal_(0, 0.1)\n",
    "    model[2].weight.data.normal_(0, 0.1)\n",
    "\n",
    "    # Generate y values using the ReLU network\n",
    "    y = model(X.view(-1, d)).view(B, N, 1)\n",
    "    y_test = model(X_test.view(-1, d)).view(B, 1).squeeze(1)\n",
    "\n",
    "    y_zero = torch.zeros(B, 1, 1).to(device)\n",
    "    X_comb = torch.cat([X, X_test], dim=1)\n",
    "    y_comb = torch.cat([y, y_zero], dim=1)\n",
    "    Z = torch.cat([X_comb, y_comb], dim=2)\n",
    "\n",
    "    return Z, y_test\n",
    "\n",
    "def generate_data_mlp(N=20, d=1, B=1000, hidden_dim=100):\n",
    "    # Generate random input data\n",
    "    X = torch.FloatTensor(B, N, d).normal_(0, 1).to(device)\n",
    "    X_test = torch.FloatTensor(B, 1, d).normal_(0, 1).to(device)\n",
    "\n",
    "    # Additional transformations if mode is 'sphere' or 'gamma' [Similar to the existing generate_data function]\n",
    "\n",
    "    # Define a 1-hidden layer ReLU network\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(d, hidden_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_dim, d)\n",
    "    ).to(device)\n",
    "    model[0].weight.data.normal_(0, 1)\n",
    "    model[2].weight.data.normal_(0, 1)\n",
    "\n",
    "    X_MLP = model(X.view(-1, d)).view(B, N, d)\n",
    "    X_test_MLP = model(X_test.view(-1, d)).view(B, 1, d)\n",
    "\n",
    "    W = torch.FloatTensor(B, d).normal_(0,1).to(device)\n",
    "    y = torch.einsum('bi,bni->bn', (W, X_MLP)).unsqueeze(2)\n",
    "    y_zero = torch.zeros(B,1,1).to(device)\n",
    "    y_test = torch.einsum('bi,bni->bn', (W, X_test_MLP)).squeeze(1)\n",
    "    X_comb= torch.cat([X_MLP,X_test_MLP],dim=1)\n",
    "    y_comb= torch.cat([y,y_zero],dim=1)\n",
    "    Z= torch.cat([X_comb,y_comb],dim=2)\n",
    "\n",
    "    return Z, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c-Rnv5HirlCm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "##############################################################################################################\n",
    "# Trains a linear Transformer with 1,2,3,4 layers\n",
    "# Plots the test loss of trained Transformer against 1,2,3,4 steps of gradient descent (with and without preconditioning)\n",
    "##############################################################################################################\n",
    "\n",
    "#use cuda if available, else use cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#torch.cuda.set_device(1)\n",
    "\n",
    "# set up some print options\n",
    "np.set_printoptions(precision = 2, suppress = True)\n",
    "torch.set_printoptions(precision=2)\n",
    "\n",
    "#begin logging\n",
    "cur_dir = 'log'\n",
    "os.makedirs(cur_dir, exist_ok=True)\n",
    "#f = open(cur_dir + '/rotation.log', \"a\", 1)\n",
    "#sys.stdout = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mE769w2br2vt"
   },
   "outputs": [],
   "source": [
    "# Set up problem parameters\n",
    "\n",
    "lr = 0.01\n",
    "clip_r = 0.01\n",
    "alg = 'adam'\n",
    "mode = 'normal'\n",
    "\n",
    "n_layer = 4  # number of layers of transformer\n",
    "N = 20     # context length\n",
    "d = 5        # dimension\n",
    "\n",
    "\n",
    "n_head = 1  # 1-headed attention\n",
    "B = 1000  # 1000 minibatch size\n",
    "var = 0.0001  # initializations scale of transformer parameter\n",
    "shape_k = 0.1  # shape_k: parameter for Gamma distributed covariates\n",
    "max_iters = 10  # Number of Iterations to run\n",
    "hist_stride = 1  # stride for saved model paramters in `train.ipynb'\n",
    "stride = 100\n",
    "\n",
    "# a convenience function for taking a step and clipping\n",
    "def clip_and_step(allparam, optimizer, clip_r = None):\n",
    "    norm_p=None\n",
    "    grad_all = allparam.grad\n",
    "    if clip_r is not None:\n",
    "        for l in range(grad_all.shape[0]):\n",
    "            for h in range(grad_all.shape[1]):\n",
    "                for t in range(grad_all.shape[2]):\n",
    "                    norm_p = grad_all[l,h,t,:,:].norm().item()\n",
    "                    if norm_p > clip_r:\n",
    "                        grad_all[l,h,t,:,:].mul_(clip_r/norm_p)\n",
    "    optimizer.step()\n",
    "    return norm_p\n",
    "\n",
    "#format for saving run data\n",
    "filename_format = '/variable_L_hist_{}_{}_{}.pth'\n",
    "n_layers = [1,2,3,4]  # number of layers of transformer\n",
    "seeds=[0,1,2,3,4]\n",
    "keys = []\n",
    "for s in seeds:\n",
    "    for n_layer in n_layers:\n",
    "        keys.append((s,n_layer,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOHBksDxsFBF",
    "outputId": "9f443d95-98b4-4718-ffe8-490e8ea64620"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "iter 0 | Loss: 4.824628829956055  time: 0.01812291145324707  gradnorm: 3.5731687545776367\n",
      "iter 1 | Loss: 4.758846759796143  time: 0.008224964141845703  gradnorm: 3.5270438194274902\n",
      "iter 2 | Loss: 4.693870544433594  time: 0.007824897766113281  gradnorm: 3.4809372425079346\n",
      "iter 3 | Loss: 4.629702568054199  time: 0.007778167724609375  gradnorm: 3.4348490238189697\n",
      "iter 4 | Loss: 4.566342830657959  time: 0.007716178894042969  gradnorm: 3.388779640197754\n",
      "(0, 2)\n",
      "iter 0 | Loss: 4.824320316314697  time: 0.017296791076660156  gradnorm: 3.5729176998138428\n",
      "iter 1 | Loss: 4.694328784942627  time: 0.017230987548828125  gradnorm: 3.4383208751678467\n",
      "iter 2 | Loss: 4.568994045257568  time: 0.017027854919433594  gradnorm: 3.307703733444214\n",
      "iter 3 | Loss: 4.448190212249756  time: 0.016848087310791016  gradnorm: 3.1810083389282227\n",
      "iter 4 | Loss: 4.331789970397949  time: 0.016844987869262695  gradnorm: 3.058178186416626\n",
      "(0, 3)\n",
      "iter 0 | Loss: 4.824447154998779  time: 0.026509761810302734  gradnorm: 3.5730161666870117\n",
      "iter 1 | Loss: 4.631755352020264  time: 0.02617502212524414  gradnorm: 3.352508068084717\n",
      "iter 2 | Loss: 4.450360298156738  time: 0.025918006896972656  gradnorm: 3.144754648208618\n",
      "iter 3 | Loss: 4.279639720916748  time: 0.02588820457458496  gradnorm: 2.9491634368896484\n",
      "iter 4 | Loss: 4.118997097015381  time: 0.026108980178833008  gradnorm: 2.765167236328125\n",
      "(0, 4)\n",
      "iter 0 | Loss: 4.82424259185791  time: 0.03592705726623535  gradnorm: 3.573093891143799\n",
      "iter 1 | Loss: 4.570348739624023  time: 0.03565788269042969  gradnorm: 3.2691550254821777\n",
      "iter 2 | Loss: 4.33693790435791  time: 0.03532910346984863  gradnorm: 2.9910991191864014\n",
      "iter 3 | Loss: 4.122336387634277  time: 0.03541922569274902  gradnorm: 2.7369437217712402\n",
      "iter 4 | Loss: 3.9249911308288574  time: 0.03532600402832031  gradnorm: 2.5048329830169678\n",
      "(1, 1)\n",
      "iter 0 | Loss: 5.347975254058838  time: 0.007957935333251953  gradnorm: 3.488400459289551\n",
      "iter 1 | Loss: 5.28257942199707  time: 0.007763862609863281  gradnorm: 3.443998336791992\n",
      "iter 2 | Loss: 5.217979431152344  time: 0.007757902145385742  gradnorm: 3.399623155593872\n",
      "iter 3 | Loss: 5.154178142547607  time: 0.00775909423828125  gradnorm: 3.3552820682525635\n",
      "iter 4 | Loss: 5.09117317199707  time: 0.007729053497314453  gradnorm: 3.310971975326538\n",
      "(1, 2)\n",
      "iter 0 | Loss: 5.3478899002075195  time: 0.01701188087463379  gradnorm: 3.4883646965026855\n",
      "iter 1 | Loss: 5.218626976013184  time: 0.01689887046813965  gradnorm: 3.358790159225464\n",
      "iter 2 | Loss: 5.093924045562744  time: 0.016772747039794922  gradnorm: 3.2330660820007324\n",
      "iter 3 | Loss: 4.973657131195068  time: 0.016904830932617188  gradnorm: 3.1111369132995605\n",
      "iter 4 | Loss: 4.857702732086182  time: 0.01684713363647461  gradnorm: 2.992947578430176\n",
      "(1, 3)\n",
      "iter 0 | Loss: 5.348346710205078  time: 0.026604175567626953  gradnorm: 3.4884777069091797\n",
      "iter 1 | Loss: 5.156671047210693  time: 0.026276111602783203  gradnorm: 3.276191473007202\n",
      "iter 2 | Loss: 4.976040363311768  time: 0.026201963424682617  gradnorm: 3.076199769973755\n",
      "iter 3 | Loss: 4.805851459503174  time: 0.02607893943786621  gradnorm: 2.8879458904266357\n",
      "iter 4 | Loss: 4.6455254554748535  time: 0.02598404884338379  gradnorm: 2.7108938694000244\n",
      "(1, 4)\n",
      "iter 0 | Loss: 5.347389221191406  time: 0.03690600395202637  gradnorm: 3.4877376556396484\n",
      "iter 1 | Loss: 5.094832897186279  time: 0.035714149475097656  gradnorm: 3.195190668106079\n",
      "iter 2 | Loss: 4.8622918128967285  time: 0.035292863845825195  gradnorm: 2.9275736808776855\n",
      "iter 3 | Loss: 4.6481404304504395  time: 0.03531599044799805  gradnorm: 2.683016538619995\n",
      "iter 4 | Loss: 4.450873851776123  time: 0.035321950912475586  gradnorm: 2.459765672683716\n",
      "(2, 1)\n",
      "iter 0 | Loss: 5.047873020172119  time: 0.008111238479614258  gradnorm: 3.4958295822143555\n",
      "iter 1 | Loss: 4.9822869300842285  time: 0.007838010787963867  gradnorm: 3.4514527320861816\n",
      "iter 2 | Loss: 4.917507171630859  time: 0.007742881774902344  gradnorm: 3.4071009159088135\n",
      "iter 3 | Loss: 4.853534698486328  time: 0.007757902145385742  gradnorm: 3.3627779483795166\n",
      "iter 4 | Loss: 4.790369033813477  time: 0.008078813552856445  gradnorm: 3.3184823989868164\n",
      "(2, 2)\n",
      "iter 0 | Loss: 5.047868251800537  time: 0.016947269439697266  gradnorm: 3.495893716812134\n",
      "iter 1 | Loss: 4.918240547180176  time: 0.017010927200317383  gradnorm: 3.3664822578430176\n",
      "iter 2 | Loss: 4.793235778808594  time: 0.0171051025390625  gradnorm: 3.2409586906433105\n",
      "iter 3 | Loss: 4.672722339630127  time: 0.017028093338012695  gradnorm: 3.119258403778076\n",
      "iter 4 | Loss: 4.556571006774902  time: 0.016933202743530273  gradnorm: 3.0013203620910645\n",
      "(2, 3)\n",
      "iter 0 | Loss: 5.047607421875  time: 0.027120113372802734  gradnorm: 3.4957680702209473\n",
      "iter 1 | Loss: 4.855457305908203  time: 0.027358055114746094  gradnorm: 3.2838141918182373\n",
      "iter 2 | Loss: 4.674495697021484  time: 0.026652812957763672  gradnorm: 3.084249496459961\n",
      "iter 3 | Loss: 4.504092216491699  time: 0.02654409408569336  gradnorm: 2.8964650630950928\n",
      "iter 4 | Loss: 4.3436479568481445  time: 0.02649974822998047  gradnorm: 2.7198731899261475\n",
      "(2, 4)\n",
      "iter 0 | Loss: 5.047438144683838  time: 0.03648996353149414  gradnorm: 3.4955084323883057\n",
      "iter 1 | Loss: 4.794229030609131  time: 0.03543591499328613  gradnorm: 3.2034339904785156\n",
      "iter 2 | Loss: 4.561286926269531  time: 0.03523516654968262  gradnorm: 2.936429262161255\n",
      "iter 3 | Loss: 4.346924304962158  time: 0.03530001640319824  gradnorm: 2.692474126815796\n",
      "iter 4 | Loss: 4.149588584899902  time: 0.03507184982299805  gradnorm: 2.469698667526245\n",
      "(3, 1)\n",
      "iter 0 | Loss: 4.988000869750977  time: 0.008115053176879883  gradnorm: 3.6705894470214844\n",
      "iter 1 | Loss: 4.920212745666504  time: 0.007946968078613281  gradnorm: 3.626267433166504\n",
      "iter 2 | Loss: 4.853213787078857  time: 0.007821083068847656  gradnorm: 3.5819599628448486\n",
      "iter 3 | Loss: 4.787005424499512  time: 0.007757902145385742  gradnorm: 3.5376663208007812\n",
      "iter 4 | Loss: 4.721589088439941  time: 0.007806062698364258  gradnorm: 3.4933905601501465\n",
      "(3, 2)\n",
      "iter 0 | Loss: 4.988579273223877  time: 0.017377138137817383  gradnorm: 3.671394109725952\n",
      "iter 1 | Loss: 4.854517459869385  time: 0.01706719398498535  gradnorm: 3.541055202484131\n",
      "iter 2 | Loss: 4.72503137588501  time: 0.017015933990478516  gradnorm: 3.4144022464752197\n",
      "iter 3 | Loss: 4.599999904632568  time: 0.017081022262573242  gradnorm: 3.2913818359375\n",
      "iter 4 | Loss: 4.479302883148193  time: 0.01721501350402832  gradnorm: 3.171947479248047\n",
      "(3, 3)\n",
      "iter 0 | Loss: 4.988414287567139  time: 0.026582956314086914  gradnorm: 3.6711881160736084\n",
      "iter 1 | Loss: 4.789592266082764  time: 0.026322126388549805  gradnorm: 3.457228422164917\n",
      "iter 2 | Loss: 4.601883888244629  time: 0.026072025299072266  gradnorm: 3.255112886428833\n",
      "iter 3 | Loss: 4.4247002601623535  time: 0.026103973388671875  gradnorm: 3.064314126968384\n",
      "iter 4 | Loss: 4.257477760314941  time: 0.026254892349243164  gradnorm: 2.884317636489868\n",
      "(3, 4)\n",
      "iter 0 | Loss: 4.98789119720459  time: 0.03539085388183594  gradnorm: 3.670506477355957\n",
      "iter 1 | Loss: 4.725798606872559  time: 0.035514116287231445  gradnorm: 3.375291109085083\n",
      "iter 2 | Loss: 4.483881950378418  time: 0.03538680076599121  gradnorm: 3.104147434234619\n",
      "iter 3 | Loss: 4.260555267333984  time: 0.035305023193359375  gradnorm: 2.8552956581115723\n",
      "iter 4 | Loss: 4.054347991943359  time: 0.03538203239440918  gradnorm: 2.6270687580108643\n",
      "(4, 1)\n",
      "iter 0 | Loss: 4.671578407287598  time: 0.00799107551574707  gradnorm: 3.5107784271240234\n",
      "iter 1 | Loss: 4.6074981689453125  time: 0.007810831069946289  gradnorm: 3.463383913040161\n",
      "iter 2 | Loss: 4.544241428375244  time: 0.007831096649169922  gradnorm: 3.416006088256836\n",
      "iter 3 | Loss: 4.481810569763184  time: 0.007704019546508789  gradnorm: 3.3686459064483643\n",
      "iter 4 | Loss: 4.420206069946289  time: 0.007735013961791992  gradnorm: 3.3213050365448\n",
      "(4, 2)\n",
      "iter 0 | Loss: 4.672964096069336  time: 0.01712775230407715  gradnorm: 3.5121350288391113\n",
      "iter 1 | Loss: 4.546316146850586  time: 0.017055749893188477  gradnorm: 3.3760039806365967\n",
      "iter 2 | Loss: 4.424347400665283  time: 0.017098188400268555  gradnorm: 3.243974447250366\n",
      "iter 3 | Loss: 4.306924819946289  time: 0.016965150833129883  gradnorm: 3.115988254547119\n",
      "iter 4 | Loss: 4.193915843963623  time: 0.016988039016723633  gradnorm: 2.991985559463501\n",
      "(4, 3)\n",
      "iter 0 | Loss: 4.672858238220215  time: 0.026927947998046875  gradnorm: 3.5121161937713623\n",
      "iter 1 | Loss: 4.485183238983154  time: 0.02619314193725586  gradnorm: 3.289849281311035\n",
      "iter 2 | Loss: 4.308809757232666  time: 0.02631378173828125  gradnorm: 3.080638885498047\n",
      "iter 3 | Loss: 4.143099308013916  time: 0.02630901336669922  gradnorm: 2.883880615234375\n",
      "iter 4 | Loss: 3.987443447113037  time: 0.026170969009399414  gradnorm: 2.698986291885376\n",
      "(4, 4)\n",
      "iter 0 | Loss: 4.672411918640137  time: 0.03571724891662598  gradnorm: 3.5118095874786377\n",
      "iter 1 | Loss: 4.425212383270264  time: 0.03518509864807129  gradnorm: 3.2059504985809326\n",
      "iter 2 | Loss: 4.198462963104248  time: 0.03531074523925781  gradnorm: 2.9265050888061523\n",
      "iter 3 | Loss: 3.990457057952881  time: 0.03517580032348633  gradnorm: 2.6714441776275635\n",
      "iter 4 | Loss: 3.7996153831481934  time: 0.0351719856262207  gradnorm: 2.438868284225464\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Determine the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for key in keys:\n",
    "    sd = key[0]\n",
    "    n_layer = key[1]\n",
    "    filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "    print(key)\n",
    "\n",
    "    prob_seed = sd\n",
    "    opt_seed = sd\n",
    "\n",
    "    hist = []\n",
    "\n",
    "    # Set seed and initialize model\n",
    "    torch.manual_seed(opt_seed)\n",
    "    model = Transformer_F(n_layer, 1, d, var, 0)\n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "    # Initialize algorithm. Important: set beta = 0.9 for adam, 0.999 is very slow\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.99, 0.9), weight_decay=0)\n",
    "\n",
    "    # Set seed\n",
    "    # Sample random rotation matrix\n",
    "    # Initialize initial training batch\n",
    "    np.random.seed(prob_seed)\n",
    "    torch.manual_seed(prob_seed)\n",
    "    gaus = torch.FloatTensor(5, 5).uniform_(-1, 1).to(device)  # Ensure tensor is on the correct device\n",
    "    U = torch.linalg.svd(gaus)[0].to(device)\n",
    "    D = torch.diag(torch.FloatTensor([1, 1, 1/2, 1/4, 1])).to(device)\n",
    "    Z, y = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "    Z = Z.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    for t in range(max_iters):\n",
    "        if t % 4000 == 0 and t > 1:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.5\n",
    "        if t % 100 == 0:\n",
    "            Z, y = generate_data_inplace(Z, U=U, D=D)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Save model parameters\n",
    "        if t % stride == 0:\n",
    "            hist.append(model.allparam.clone().detach())\n",
    "\n",
    "        loss = in_context_loss(model, Z, y)\n",
    "\n",
    "        # Compute gradient and take a step\n",
    "        loss.backward()\n",
    "        norms = clip_and_step(model.allparam, optimizer, clip_r=clip_r)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if t % 100 == 0 or t < 5:\n",
    "            print('iter {} | Loss: {}  time: {}  gradnorm: {}'.format(t, loss.item(), end-start, norms))\n",
    "\n",
    "    torch.save({'hist': hist, 'U': U, 'D': D}, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvQIjkamGN9X",
    "outputId": "cfe8776f-a3da-44d2-b510-f66f730f43d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/b8j10z1d3db8wtgxyks8jk1m0000gp/T/ipykernel_83675/1465035067.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  hist = torch.load(filename)['hist']\n",
      "/var/folders/_g/b8j10z1d3db8wtgxyks8jk1m0000gp/T/ipykernel_83675/1465035067.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  U = torch.load(filename)['U']\n",
      "/var/folders/_g/b8j10z1d3db8wtgxyks8jk1m0000gp/T/ipykernel_83675/1465035067.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  D = torch.load(filename)['D']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m in_context_loss(model, Z_val, y_val)\n\u001b[0;32m---> 36\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Generate new test data after fine-tuning gamma\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "# compute test loss for trained linear Transformers\n",
    "########################################################\n",
    "loss_dict_zero = {}\n",
    "store = 0\n",
    "for sd in seeds:\n",
    "    key = (sd,)\n",
    "    loss_dict_zero[key] = torch.zeros(4)\n",
    "    for n_layer in n_layers:\n",
    "        # Load parameters for given n_layer and seed\n",
    "        filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "        hist = torch.load(filename)['hist']\n",
    "        U = torch.load(filename)['U']\n",
    "        D = torch.load(filename)['D']\n",
    "\n",
    "        # Validation set to find the best model and fine-tune gamma_param\n",
    "        np.random.seed(999)\n",
    "        torch.manual_seed(999)\n",
    "        Z_val, y_val = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "        Z_val = Z_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        model = Transformer_F(n_layer, n_head, d, var, 0).to(device)\n",
    "\n",
    "        # Fine-tune gamma on the validation data\n",
    "        model.allparam.requires_grad = True\n",
    "\n",
    "        model.allparam.data.copy_(hist[-1])\n",
    "\n",
    "        # Use Adam optimizer for fine-tuning\n",
    "        optimizer = torch.optim.Adam([model.allparam], lr=lr)\n",
    "\n",
    "        fine_tune_iters = 1000\n",
    "        for t in range(fine_tune_iters):  # fine_tune_iters: number of fine-tuning steps\n",
    "            optimizer.zero_grad()\n",
    "            loss = in_context_loss(model, Z_val, y_val)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Generate new test data after fine-tuning gamma\n",
    "        np.random.seed(99)\n",
    "        torch.manual_seed(99)\n",
    "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "        Z_test = Z_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        # Compute loss after fine-tuning and on the new test data\n",
    "        with torch.no_grad():\n",
    "            loss_dict_zero[key][n_layer - 1] = in_context_loss(model, Z_test, y_test).log().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JSJdGi-EvbAh",
    "outputId": "74fddc6d-a4e4-4809-8033-6cd4822cd5ea"
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# compute test loss for trained linear Transformers\n",
    "########################################################\n",
    "loss_dict = {}\n",
    "store = 0\n",
    "for sd in seeds:\n",
    "    key = (sd,)\n",
    "    loss_dict[key] = torch.zeros(4)\n",
    "    for n_layer in n_layers:\n",
    "        # Load parameters for given n_layer and seed\n",
    "        filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "        hist = torch.load(filename)['hist']\n",
    "        U = torch.load(filename)['U']\n",
    "        D = torch.load(filename)['D']\n",
    "\n",
    "        # Validation set to find the best model and fine-tune gamma_param\n",
    "        np.random.seed(999)\n",
    "        torch.manual_seed(999)\n",
    "        Z_val, y_val = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "        Z_val = Z_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        model = Transformer_F(n_layer, n_head, d, var, 1).to(device)\n",
    "\n",
    "        # Fine-tune gamma on the validation data\n",
    "        model.alpha.requires_grad = True\n",
    "        model.gamma.requires_grad = True\n",
    "        model.allparam.requires_grad = True\n",
    "\n",
    "        model.allparam.data.copy_(hist[-1])\n",
    "\n",
    "        # Use Adam optimizer for fine-tuning\n",
    "        optimizer = torch.optim.Adam([model.allparam, model.alpha, model.gamma], lr=lr)\n",
    "\n",
    "        fine_tune_iters = 1000\n",
    "        for t in range(fine_tune_iters):  # fine_tune_iters: number of fine-tuning steps\n",
    "            optimizer.zero_grad()\n",
    "            loss = in_context_loss(model, Z_val, y_val)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Generate new test data after fine-tuning gamma\n",
    "        np.random.seed(99)\n",
    "        torch.manual_seed(99)\n",
    "        Z_test, y_test = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "        Z_test = Z_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        # Compute loss after fine-tuning and on the new test data\n",
    "        with torch.no_grad():\n",
    "            loss_dict[key][n_layer - 1] = in_context_loss(model, Z_test, y_test).log().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkuKre0QtE3t",
    "outputId": "1e4fa7ad-c8e7-4aea-99dd-bb1340cff665"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Conjugate Gradient Descent function\n",
    "def do_cgd(Z, eta, numstep):\n",
    "    N = Z.shape[0] - 1\n",
    "    X = Z[0:N-1, 0:5]\n",
    "    Y = Z[0:N-1, 5]\n",
    "    w = torch.zeros(X.shape[1]).to(device)\n",
    "\n",
    "    # Initial gradient and direction\n",
    "    r = torch.einsum('ik,ij,j->k', X, X, w) - torch.einsum('ik,i->k', X, Y)\n",
    "    p = -r\n",
    "\n",
    "    for k in range(numstep):\n",
    "        Xp = torch.einsum('ik,ij,j->k', X, X, p)\n",
    "        alpha = torch.dot(r, r) / torch.dot(p, Xp)\n",
    "        w = w + alpha * p  # Update the weight\n",
    "\n",
    "        r_new = r + alpha * Xp\n",
    "        beta = torch.dot(r_new, r_new) / torch.dot(r, r)\n",
    "        p = -r_new + beta * p  # Update direction\n",
    "\n",
    "        r = r_new\n",
    "\n",
    "    return w\n",
    "\n",
    "# Evaluation function for an instance\n",
    "def eval_w_instance(Z, Ytest, w):\n",
    "    N = Z.shape[0] - 1\n",
    "    Xtest = Z[N, 0:5]\n",
    "    prediction = torch.einsum('i,i->', w, Xtest)\n",
    "    return (Ytest - prediction)**2, prediction\n",
    "\n",
    "# Initialization of loss matrix\n",
    "gd_loss_matrix = torch.zeros(len(seeds), 4)\n",
    "\n",
    "# Main loop to find the best eta and evaluate performance\n",
    "for n_layer in n_layers:\n",
    "    # First, find the best eta\n",
    "    sd = 1\n",
    "    best_loss = 10000\n",
    "    best_eta = 0\n",
    "    numstep = n_layer\n",
    "\n",
    "    # Load UD matrices\n",
    "    filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "    U = torch.load(filename)['U']\n",
    "    D = torch.load(filename)['D']\n",
    "\n",
    "    # Generate test data using seed 999\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    Z, y = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "    Z = Z.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # Done generating data\n",
    "\n",
    "    print(\"Checkpoint 1\")\n",
    "    for eta in [0.008, 0.01, 0.02, 0.04, 0.08, 0.16]:\n",
    "        ### Start of evaluate mean loss ###\n",
    "        total_loss = 0\n",
    "        for i in range(Z.shape[0]):\n",
    "            Zi = Z[i, :, :]\n",
    "            Ytesti = y[i]\n",
    "            w = do_cgd(Zi, eta, numstep)  # Use do_cgd instead of do_gd\n",
    "            gd_loss, gd_pred = eval_w_instance(Zi, Ytesti, w)\n",
    "            total_loss = total_loss + gd_loss\n",
    "        mean_loss = total_loss / 5000\n",
    "        ### End of evaluate mean loss ###\n",
    "        print('eta: {}, loss: {}'.format(eta, mean_loss))\n",
    "        if (mean_loss < best_loss):\n",
    "            best_eta = eta\n",
    "            best_loss = mean_loss\n",
    "    print('best eta: {} for n_layer={}'.format(best_eta, n_layer))\n",
    "\n",
    "    print(\"Checkpoint 2\")\n",
    "    print(seeds)\n",
    "    # Now do actual evaluation\n",
    "    for sd in seeds:\n",
    "        opt_seed = sd\n",
    "\n",
    "        filename = cur_dir + filename_format.format(n_layer, N, sd)\n",
    "        U = torch.load(filename)['U']\n",
    "        D = torch.load(filename)['D']\n",
    "\n",
    "        # Generate test data\n",
    "        torch.manual_seed(sd)\n",
    "        Z, y = generate_data(mode, N, d, B, shape_k, U, D)\n",
    "        Z = Z.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Done generating data\n",
    "        eta = best_eta\n",
    "\n",
    "        ### Start of evaluate mean loss ###\n",
    "        total_loss = 0\n",
    "        for i in range(Z.shape[0]):\n",
    "            Zi = Z[i, :, :]\n",
    "            Ytesti = y[i]\n",
    "            w = do_cgd(Zi, eta, numstep)  # Use do_cgd instead of do_gd\n",
    "            gd_loss, gd_pred = eval_w_instance(Zi, Ytesti, w)\n",
    "            total_loss = total_loss + gd_loss\n",
    "        mean_loss = total_loss / Z.shape[0]\n",
    "        gd_loss_matrix[sd, n_layer-1] = mean_loss\n",
    "\n",
    "print(\"Checkpoint 3\")\n",
    "# Compute mean and std of log test loss for plotting\n",
    "gd_loss_mean = gd_loss_matrix.log().mean(dim=0)\n",
    "gd_loss_std = gd_loss_matrix.log().var(dim=0)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "id": "HmXIuGJNt1VB",
    "outputId": "2979e614-d5d9-4d95-e47e-ed3adfcc57ff"
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# plot final test loss against N\n",
    "####################################\n",
    "\n",
    "fig_dir = 'figures'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1,figsize = (9, 9))\n",
    "\n",
    "losses = torch.zeros(len(seeds), len(n_layers))\n",
    "keys = loss_dict.keys()\n",
    "for idx, key in enumerate(keys):\n",
    "    losses[idx,:] = loss_dict[key]\n",
    "losses_mean = torch.mean(losses, axis=0)\n",
    "losses_std = torch.std(losses, axis=0)/10\n",
    "\n",
    "losses_zero = torch.zeros(len(seeds), len(n_layers))\n",
    "keys = loss_dict_zero.keys()\n",
    "for idx, key in enumerate(keys):\n",
    "    losses_zero[idx,:] = loss_dict_zero[key]\n",
    "losses_mean_zero = torch.mean(losses_zero, axis=0)\n",
    "losses_std_zero = torch.std(losses_zero, axis=0)/10\n",
    "\n",
    "plt.plot(n_layers, gd_loss_mean, color='blue', label='Conjugate Gradient Descent')\n",
    "plt.fill_between(n_layers, gd_loss_mean - gd_loss_std/10, gd_loss_mean + gd_loss_std/10, color='blue', alpha=0.2)\n",
    "\n",
    "ax.plot(n_layers, losses_mean, color = 'red', lw = 3, label='Memformer with CGD')\n",
    "ax.fill_between(n_layers, losses_mean-losses_std, losses_mean+losses_std, color = 'red', alpha = 0.2)\n",
    "\n",
    "ax.plot(n_layers, losses_mean_zero, color = 'green', lw = 3, label='Linear Transformer')\n",
    "ax.fill_between(n_layers, losses_mean_zero-losses_std_zero, losses_mean_zero+losses_std_zero, color = 'green', alpha = 0.2)\n",
    "\n",
    "plt.ylabel('log(Loss)',fontsize=30)\n",
    "plt.xlabel('Number of Layers/Steps',fontsize=30)\n",
    "ax.tick_params(axis='both', which='major', labelsize=30, width = 3, length = 10)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=20, width = 3, length = 5)\n",
    "ax.legend(fontsize=24)\n",
    "#ax.set_yscale('log')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir + '/variable-L-plot.pdf', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOpJIPH18t7z+BxIxxEsWju",
   "gpuType": "T4",
   "mount_file_id": "1DKMW_CHuhRiXb5B4iCi6e8r-t_QLExHb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
